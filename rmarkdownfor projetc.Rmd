---
title: "Coursera Practicial Machine Learning Course Project"
author: "Hazem"
date: "14 July 2016"
output: html_document
---

```{r setup, include=FALSE}
library(caret)
library(rpart)
```

## Overview
This report is submitted for Coursera Practical Machine Learning course Peer Assessed Project. The goal of the project is to use accelerometers data to predict how health participants performed a unilateral dumbbell bicep curl exercise. The analysis is done using the caret package.

##The Data

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human &#39;13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Dataset description from the authors':

"Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate.    

##The Challenge

By using data gathered from accelerometers on the belt, forearm, arm, and dumbbell of the 6 health participants, build a a machine learning algorithm in order to predict the appropriate activity quality (class A-E)?

##Getting and Cleaning data

```
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```


Removing columns with NA values


```
nonNAs <- function(x) {
       as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}

# number of non NAs in a columns
 colNacnts <- nonNAs(training)
 
 #columns to be deleted
 delCols <- c()

 # the columns will either have low nans or very few values, 
 # so if the column is not complete then delete it
 for (cnt in 1:160) {
         if (colNacnts[cnt] < nrow(training)) {
           delCols <- c(delCols, colnames(training)[cnt])
         }
 }

```

 remove the columns and remove row identification columns

```
 training <- training[,!(names(training) %in% delCols)]
 training <- training[, -c(1,3,4,5,6,7)]
 
 testing <- testing[,!(names(testing) %in% delCols)]
 testing <- testing[, -c(1,3,4,5,6,7)]
```
 
##Non Zero Varience
 
After the previous step, i tested for nonZeroVarience but it turns out it is unnseccery to remove more columns due to invariability
    
```
#                          freqRatio percentUnique zeroVar   nzv
# roll_belt                1.102       6.77811   FALSE FALSE
# pitch_belt               1.036       9.37723   FALSE FALSE
# yaw_belt                 1.058       9.97350   FALSE FALSE
# total_accel_belt         1.063       0.14779   FALSE FALSE
# gyros_belt_x             1.059       0.71348   FALSE FALSE
# gyros_belt_y             1.144       0.35165   FALSE FALSE
# gyros_belt_z             1.066       0.86128   FALSE FALSE
# accel_belt_x             1.055       0.83580   FALSE FALSE
# accel_belt_y             1.114       0.72877   FALSE FALSE
# accel_belt_z             1.079       1.52380   FALSE FALSE
# magnet_belt_x            1.090       1.66650   FALSE FALSE
# magnet_belt_y            1.100       1.51870   FALSE FALSE
# magnet_belt_z            1.006       2.32902   FALSE FALSE
# roll_arm                52.338      13.52563   FALSE FALSE
# pitch_arm               87.256      15.73234   FALSE FALSE
# yaw_arm                 33.029      14.65702   FALSE FALSE
# total_accel_arm          1.025       0.33636   FALSE FALSE
# gyros_arm_x              1.016       3.27693   FALSE FALSE
# gyros_arm_y              1.454       1.91622   FALSE FALSE
# gyros_arm_z              1.111       1.26389   FALSE FALSE
# accel_arm_x              1.017       3.95984   FALSE FALSE
# accel_arm_y              1.140       2.73672   FALSE FALSE
# accel_arm_z              1.128       4.03629   FALSE FALSE
# magnet_arm_x             1.000       6.82397   FALSE FALSE
# magnet_arm_y             1.057       4.44399   FALSE FALSE
# magnet_arm_z             1.036       6.44685   FALSE FALSE
# roll_dumbbell            1.022      83.78351   FALSE FALSE
# pitch_dumbbell           2.277      81.22516   FALSE FALSE
# yaw_dumbbell             1.132      83.14137   FALSE FALSE
# total_accel_dumbbell     1.073       0.21914   FALSE FALSE
# gyros_dumbbell_x         1.003       1.22821   FALSE FALSE
# gyros_dumbbell_y         1.265       1.41678   FALSE FALSE
# gyros_dumbbell_z         1.060       1.04984   FALSE FALSE
# accel_dumbbell_x         1.018       2.16594   FALSE FALSE
# accel_dumbbell_y         1.053       2.37489   FALSE FALSE
# accel_dumbbell_z         1.133       2.08949   FALSE FALSE
# magnet_dumbbell_x        1.098       5.74865   FALSE FALSE
# magnet_dumbbell_y        1.198       4.30129   FALSE FALSE
# magnet_dumbbell_z        1.021       3.44511   FALSE FALSE
# roll_forearm            11.589      11.08959   FALSE FALSE
# pitch_forearm           65.983      14.85577   FALSE FALSE
# yaw_forearm             15.323      10.14677   FALSE FALSE
# total_accel_forearm      1.129       0.35674   FALSE FALSE
# gyros_forearm_x          1.059       1.51870   FALSE FALSE
# gyros_forearm_y          1.037       3.77637   FALSE FALSE
# gyros_forearm_z          1.123       1.56457   FALSE FALSE
# accel_forearm_x          1.126       4.04648   FALSE FALSE
# accel_forearm_y          1.059       5.11161   FALSE FALSE
# accel_forearm_z          1.006       2.95587   FALSE FALSE
# magnet_forearm_x         1.012       7.76679   FALSE FALSE
# magnet_forearm_y         1.247       9.54031   FALSE FALSE
# magnet_forearm_z         1.000       8.57711   FALSE FALSE
# classe                   1.470       0.02548   FALSE FALSE

```

##Splitting the data into training and Validation sets

```
 inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
 TrainSet <- training[inTrain, ]
 TestSet  <- training[-inTrain, ]
```
 
##Bulding Models

## 1- Classification Tree

I decided to start with a classification tree with the train function without cross validation and then test the model with the validation set

```
 modFitRPart <- train(classe ~ ., data=TrainSet, method="rpart")
 predictRPart <- predict(modFitRPart, newdata=TestSet)

 confMatRPart <- confusionMatrix(predictRPart, TestSet$classe)
 confMatRPart
```
 
 The following results shows a very poor performance with a 0.4867 accurecy
 
## 2- Classification Tree with cross validation

I decided to use cross validation hoping for an imporved perfomance

```
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
 modFitRPartCVPP <- train(classe ~ ., data=TrainSet, method="rpart", preProcess=c("center", "scale"),trControl=controlRF)
 
 predictRPartCVPP <- predict(modFitRPartCVPP, newdata=TestSet)
 confMatRPartCVPP <- confusionMatrix(predictRPartCVPP, TestSet$classe)
 confMatRPartCVPP
```
 
 Perfomance improved slightly to  0.4904 
 
## 3- Decision tree with cross validation and pre processing
Next I tried to include pre-processing but it had no effect on performance

```
modFitDecTreeCVPP <- train(classe ~ ., data=TrainSet, method="rpart" , preProcess=c("center", "scale" ),trControl=controlRF)>    
predictDecTreeCVPP <- predict(modFitDecTreeCVPP, newdata=TestSet)>    
confMatDecTreeCVPP <- confusionMatrix(predictDecTreeCVPP, TestSet$classe)>    
confMatDecTreeCVPP
```

## 4- Using the rpart function
I tried training the model by running the rpart function directly, and sureprisingly perofmance increased to 0.7193

```
modFitDecTree <- modFitDecTree <- rpart(classe ~ ., data=TrainSet, method="class")
predictDecTree <- predict(modFitDecTree, newdata=TestSet)
confMatDecTree <- confusionMatrix(predictDecTree, TestSet$classe)
confMatDecTree
```

##Random forest without cross validation
After the poor performance of Decision trees i Decided to train the model using Random Forest Trees

```
#modFitRandForest <- train(classe ~ ., data=TrainSet, method="rf")
#predictRandForest <- predict(modFitRandForest, newdata=TestSet)
#confMatRandForest <- confusionMatrix(predictRandForest, TestSet$classe)
#onfMatRandForest
```

This has resulted in a great performance of 0.9913. However, the processing time is very taxing</p>

```
#Confusion Matrix and Statistics
#          Reference
#Prediction    A    B    C    D    E
#         A 1672   11    0    0    0
#         B    1 1124    3    0    1
#         C    0    4 1019   16    5
#         D    0    0    4  946    3
#         E    1    0    0    2 1073
#Overall Statistics
#                                          
#               Accuracy : 0.9913          
#                 95% CI : (0.9886, 0.9935)
#    No Information Rate : 0.2845          
#    P-Value [Acc > NIR] : < 2.2e-16       
#                                          
#                  Kappa : 0.989           
# Mcnemar's Test P-Value : NA              
# Statistics by Class:#
#                     Class: A Class: B Class: C Class: D Class: E
#Sensitivity            0.9988   0.9868   0.9932   0.9813   0.9917
#Specificity            0.9974   0.9989   0.9949   0.9986   0.9994
#Pos Pred Value         0.9935   0.9956   0.9761   0.9927   0.9972
#Neg Pred Value         0.9995   0.9968   0.9986   0.9964   0.9981
#Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
#Detection Rate         0.2841   0.1910   0.1732   0.1607   0.1823
#Detection Prevalence   0.2860   0.1918   0.1774   0.1619   0.1828
#Balanced Accuracy      0.9981   0.9929   0.9940   0.9900   0.9955
```

##Random forest with cross validation
I then used cross validation which resluted in improving the performance from 0.9913 to 0.9915

```
#controlRF <- trainControl(method="cv" , number=3, verboseIter=FALSE)
#modFitRandForest <- train(classe ~ ., data=TrainSet, method="rf",trControl=controlRF)
#predictRandForest <- predict(modFitRandForest, newdata=TestSet)
#confMatRandForest <- confusionMatrix(predictRandForest, TestSet$classe)
#confMatRandForest
```
```
#Confusion Matrix and Statistics
#          Reference
#Prediction    A    B    C    D    E
#         A 1672   10    0    0    0
#         B    1 1125    3    0    1
#         C    0    4 1018   16    5
#         D    0    0    5  947    3
#         E    1    0    0    1 1073
#Overall Statistics
#
#               Accuracy : 0.9915
#                 95% CI : (0.9888, 0.9937)
#    No Information Rate : 0.2845
#    P-Value [Acc > NIR] : < 2.2e-16
#
#                  Kappa : 0.9893
# Mcnemar's Test P-Value : NA
#Statistics by Class:
#                     Class: A Class: B Class: C Class: D Class: E
#Sensitivity            0.9988   0.9877   0.9922   0.9824   0.9917
#Specificity            0.9976   0.9989   0.9949   0.9984   0.9996
#Pos Pred Value         0.9941   0.9956   0.9760   0.9916   0.9981
#Neg Pred Value         0.9995   0.9971   0.9983   0.9966   0.9981
#Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
#Detection Rate         0.2841   0.1912   0.1730   0.1609   0.1823
#Detection Prevalence   0.2858   0.1920   0.1772   0.1623   0.1827
#Balanced Accuracy      0.9982   0.9933   0.9935   0.9904   0.9956
```

##Out Of Sample Error
Decision Tree (using train function, no cross validation or pre-preocessing)        0.4867 = 0.5133
Decision Tree (using train function, with cross validation)                         0.4904 = 0.5096
Decision Tree (using train function, with cross validation and pre-preocessing)     0.4904 = 0.5096
Decision Tree (using rpart function, no cross validation or pre-preocessing)        0.7193 = 0.2807
Random Forest (no cross validation)                                                 0.9913 = 0.0087
Random Forest (cross validation)                                                    0.9915 = 0.0085
   
##Model Chossing and testing
Finally, the choosen model "Random Forest with Cross Validation" needs to run on the test data to predict the outcome of 20 different test cases. I ran the model and submitted the results throught the quiz.



